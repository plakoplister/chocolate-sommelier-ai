import OpenAI from 'openai'
import { IntelligentSommelier } from './intelligent-sommelier.js'

export class HybridOpenAISommelier extends IntelligentSommelier {
  constructor() {
    super()

    if (process.env.OPENAI_API_KEY) {
      this.openai = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY
      })
      this.useOpenAI = true
    } else {
      this.useOpenAI = false
      console.log('‚ö†Ô∏è OpenAI API key not found, using intelligent local sommelier')
    }
  }

  async processUserMessage({ message, currentPreferences, conversationHistory }) {
    // Si pas d'OpenAI, utiliser la logique locale intelligente
    if (!this.useOpenAI) {
      return await this.processWithLocalIntelligence({ message, currentPreferences, conversationHistory })
    }

    try {
      // √âTAPE 1: OpenAI analyse le message et extrait les pr√©f√©rences
      const analysis = await this.analyzeMessageWithAI(message, conversationHistory, currentPreferences)

      // √âTAPE 2: Si pr√™t pour la recherche, utiliser la logique locale rapide
      if (analysis.readyForSearch) {
        const recommendations = this.findRecommendationsLocal(analysis.extractedPreferences)

        // √âTAPE 3: OpenAI pr√©sente les r√©sultats naturellement
        return await this.presentRecommendationsWithAI(recommendations, message, analysis)
      }

      // Continue la conversation avec OpenAI
      return {
        message: analysis.conversationalResponse,
        preferences: analysis.extractedPreferences,
        recommendations: []
      }

    } catch (error) {
      console.error('OpenAI API error:', error)
      // Fallback vers logique locale intelligente
      return await this.processWithLocalIntelligence({ message, currentPreferences, conversationHistory })
    }
  }

  /**
   * √âTAPE 1: Analyse du message par OpenAI
   */
  async analyzeMessageWithAI(message, conversationHistory, currentPreferences) {
    const systemPrompt = this.createAnalysisPrompt()

    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory.map(msg => ({
        role: msg.type === 'user' ? 'user' : 'assistant',
        content: msg.content
      })),
      { role: 'user', content: `PR√âF√âRENCES ACTUELLES: ${JSON.stringify(currentPreferences)}\nMESSAGE: ${message}` }
    ]

    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages,
      temperature: 0.7,
      max_tokens: 500
    })

    let response
    try {
      const content = completion.choices[0].message.content
      // Try to parse as JSON first
      if (content.includes('{')) {
        const jsonMatch = content.match(/\{[\s\S]*\}/)
        response = jsonMatch ? JSON.parse(jsonMatch[0]) : { conversationalResponse: content }
      } else {
        response = { conversationalResponse: content }
      }
    } catch (e) {
      // Fallback to text response
      response = { conversationalResponse: completion.choices[0].message.content }
    }

    // Fusionner avec les pr√©f√©rences existantes
    const extractedPreferences = {
      ...currentPreferences,
      ...response.extractedPreferences
    }

    // Analyser avec la logique intelligente locale
    const intelligentAnalysis = this.analyzePreferences(extractedPreferences, conversationHistory)

    return {
      ...response,
      extractedPreferences,
      readyForSearch: intelligentAnalysis.readyForSearch,
      confidence: intelligentAnalysis.confidence,
      userExpertise: intelligentAnalysis.userExpertise
    }
  }

  /**
   * √âTAPE 3: Pr√©sentation des recommandations par OpenAI
   */
  async presentRecommendationsWithAI(recommendations, originalMessage, analysis) {
    const searchContext = this.generateSearchContext(analysis.extractedPreferences, recommendations)
    const presentationPrompt = this.createPresentationPrompt(searchContext, analysis.userExpertise)

    const messages = [
      { role: 'system', content: presentationPrompt },
      { role: 'user', content: `MESSAGE ORIGINAL: "${originalMessage}"\n\nRECOMMANDATIONS TROUV√âES:\n${JSON.stringify(recommendations.slice(0, 3), null, 2)}` }
    ]

    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages,
      temperature: 0.8,
      max_tokens: 600
    })

    return {
      message: completion.choices[0].message.content,
      preferences: analysis.extractedPreferences,
      recommendations: recommendations.slice(0, 5)
    }
  }

  /**
   * Prompt pour l'analyse intelligente des messages
   */
  createAnalysisPrompt() {
    return `Tu es un sommelier chocolat expert qui analyse les messages utilisateur pour extraire leurs pr√©f√©rences.

TA MISSION: Analyser le message et extraire PR√âCIS√âMENT les pr√©f√©rences chocolat.

PR√âF√âRENCES √Ä EXTRAIRE:
- cocoa_percentage: ["light" (32-60%), "medium" (65-75%), "dark" (80%+)] ou pourcentages exacts
- flavor_profile: ["fruit√©", "√©pic√©", "floral", "terreux", "noisette", "vanill√©", "chocolat√©", "caramel"]
- origin_preference: ["Madagascar", "Venezuela", "√âquateur", "P√©rou", "Ghana", "Trinidad", etc.]
- budget: nombre en euros (ex: 25)
- occasion: "d√©gustation", "cadeau", "p√¢tisserie", "quotidien", "sp√©cial"
- texture_preference: ["creamy", "smooth", "crisp", "grainy", "silky"]
- intensity: "low", "medium", "high"

EXPERTISE UTILISATEUR:
- "novice": mots comme "d√©couvrir", "commencer", "d√©butant"
- "expert": mots comme "terroir", "fermentation", "criollo"
- "intermediate": par d√©faut

D√âTERMINER readyForSearch:
- true: si au moins 2 crit√®res pr√©cis sont identifi√©s
- false: si informations insuffisantes, pose UNE question cibl√©e

R√âPONSE JSON OBLIGATOIRE:
{
  "extractedPreferences": {
    // pr√©f√©rences extraites
  },
  "readyForSearch": boolean,
  "conversationalResponse": "ta r√©ponse naturelle si readyForSearch=false",
  "reasoning": "pourquoi tu as pris cette d√©cision"
}

EXEMPLES:
Message: "Je veux d√©couvrir le chocolat"
‚Üí readyForSearch: false, pose question sur l'intensit√© pr√©f√©r√©e

Message: "Chocolat noir 70% fruit√© pour cadeau budget 20‚Ç¨"
‚Üí readyForSearch: true, extrait tout

Sois naturel, expert et bienveillant dans tes r√©ponses !`
  }

  /**
   * Prompt pour la pr√©sentation des recommandations
   */
  createPresentationPrompt(searchContext, userExpertise) {
    return `Tu es un sommelier chocolat expert de XOCOA qui pr√©sente des recommandations personnalis√©es.

CONTEXTE DE RECHERCHE:
- Crit√®res utilis√©s: ${searchContext.searchCriteria.join(', ')}
- R√©sultats: ${searchContext.resultsSummary.count} chocolats trouv√©s
- Niveau utilisateur: ${userExpertise}
- Confiance: ${searchContext.confidence}

TON STYLE selon l'expertise:
- NOVICE: Explications simples, focus sur plaisir, rassurer
- EXPERT: D√©tails techniques, terroir, processus de fabrication
- INTERMEDIATE: √âquilibre entre plaisir et connaissances

STRUCTURE DE R√âPONSE:
1. Accroche personnalis√©e bas√©e sur leur demande
2. Pr√©sentation de 3 chocolats MAX avec:
   - Nom et maker
   - Pourquoi c'est parfait pour eux
   - 1-2 caract√©ristiques cl√©s adapt√©es √† leur niveau
3. Conseil de d√©gustation ou suggestion

STYLE:
- Chaleureux et expert
- Enthousiaste mais pas excessif
- Personnalis√© selon leurs pr√©f√©rences exactes
- Fran√ßais naturel

√âvite les listes √† puces, privil√©gie un discours fluide.
Ne mentionne PAS les scores techniques ou prix exactement, int√®gre-les naturellement.`
  }

  /**
   * Fallback: Traitement avec logique locale intelligente
   */
  async processWithLocalIntelligence({ message, currentPreferences, conversationHistory }) {
    // Extraction locale am√©lior√©e
    const extractedPreferences = this.extractPreferencesEnhanced(message)
    const updatedPreferences = { ...currentPreferences, ...extractedPreferences }

    // Analyse intelligente
    const analysis = this.analyzePreferences(updatedPreferences, conversationHistory)

    if (analysis.readyForSearch) {
      // Recherche locale
      const recommendations = this.findRecommendationsLocal(updatedPreferences)

      // R√©ponse locale intelligente
      const responseMessage = this.generateIntelligentResponse(recommendations, updatedPreferences, analysis.userExpertise)

      return {
        message: responseMessage,
        preferences: updatedPreferences,
        recommendations: recommendations.slice(0, 5)
      }
    }

    // Continue la conversation localement
    const nextQuestion = this.generateIntelligentQuestion(updatedPreferences, analysis.userExpertise)

    return {
      message: nextQuestion,
      preferences: updatedPreferences,
      recommendations: []
    }
  }

  /**
   * G√©n√®re une r√©ponse intelligente locale pour les recommandations
   */
  generateIntelligentResponse(recommendations, preferences, expertise) {
    if (!recommendations.length) {
      return "Je n'ai pas trouv√© de chocolats correspondant exactement √† vos crit√®res. Pouvez-vous ajuster votre budget ou vos pr√©f√©rences ?"
    }

    let response = ""

    // Accroche selon le contexte
    if (preferences.occasion === 'cadeau') {
      response += "Parfait pour un cadeau ! "
    } else if (preferences.expertise === 'novice') {
      response += "Excellente d√©couverte en perspective ! "
    } else {
      response += "Voici ma s√©lection d'exception : "
    }

    // Pr√©sentation des chocolats
    recommendations.slice(0, 3).forEach((chocolate, index) => {
      const prefix = index === 0 ? "\n\nüç´ " : "\n\nüç´ "
      response += `${prefix}**${chocolate.name}** par ${chocolate.maker_name || 'Artisan'}`

      if (chocolate.origin_country) {
        response += ` (${chocolate.origin_country})`
      }

      response += ` - ${chocolate.cocoa_percentage}% cacao`

      if (chocolate.flavor_notes_primary) {
        response += `. Notes de ${chocolate.flavor_notes_primary.toLowerCase()}`
      }

      if (chocolate.price_retail) {
        response += ` - ${chocolate.price_retail}‚Ç¨`
      }
    })

    // Conseil final selon l'expertise
    if (expertise === 'novice') {
      response += "\n\nCommencez par le premier, il est parfait pour d√©couvrir les saveurs authentiques !"
    } else if (expertise === 'expert') {
      response += "\n\nChacun r√©v√®le un terroir unique. Je recommande une d√©gustation comparative !"
    }

    return response
  }

  /**
   * G√©n√®re une question intelligente locale
   */
  generateIntelligentQuestion(preferences, expertise) {
    const missing = []

    if (!preferences.cocoa_percentage) missing.push('intensit√©')
    if (!preferences.flavor_profile) missing.push('profil aromatique')
    if (!preferences.budget) missing.push('budget')
    if (!preferences.occasion) missing.push('occasion')

    if (!missing.length) {
      return "Parfait ! Laissez-moi chercher les chocolats id√©aux pour vous..."
    }

    const nextCriteria = missing[0]

    switch (nextCriteria) {
      case 'intensit√©':
        return expertise === 'novice'
          ? "Pour commencer, pr√©f√©rez-vous quelque chose de doux et cr√©meux, ou √™tes-vous pr√™t(e) pour des saveurs plus intenses ?"
          : "Quelle intensit√© de cacao recherchez-vous ? Plut√¥t √©quilibr√© autour de 70%, ou plus cors√© avec 80%+ ?"

      case 'profil aromatique':
        return "C√¥t√© saveurs, √™tes-vous plut√¥t attir√©(e) par des notes fruit√©es, √©pic√©es, ou peut-√™tre quelque chose de plus terreux et authentique ?"

      case 'budget':
        return "Quel budget souhaitez-vous consacrer √† cette d√©couverte ? (15‚Ç¨, 25‚Ç¨, ou plus ?)"

      case 'occasion':
        return "C'est pour quelle occasion ? D√©gustation personnelle, cadeau, ou pour accompagner un dessert ?"

      default:
        return "Dites-moi en plus sur vos go√ªts en chocolat !"
    }
  }
}